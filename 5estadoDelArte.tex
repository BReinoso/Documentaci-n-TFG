\label{chap:EstadoArte}
En este apartado se procede a la presentación de una serie de artículos que han conformado el estudio teórico que conlleva este proyecto. el cuál tiene un gran peso en el mismo; pues el estudio de cada una de las herramientas que se han tenido en cuenta tiene por detrás un concienzudo estudio por parte del alumno, el cuál ha tenido que comprender el funcionamiento de las mismas a través de este estudio.

Para realizar la explicación de los artículos, estos serán traducidos de manera resumida, ordenada y eliminando los aspectos que sean demasiado avanzados o que no se consideren del todo necesaria su explicación en este proyecto.
\section{Artículo del DeepBelief SDK}
\label{subchap:EstadoDeepBelief}
El artículo de investigación que se va a explicar es el que se ha usado para implementar la herramienta \textit{Deep Belief SDK}\footnote{\url{https://github.com/jetpacapp/DeepBeliefSDK}} y se puede encontrar con el nombre de \textit{ImageNet Classification with Deep Convolutional Neural Networks}\cite{NIPS2012_4824}. Lo siguiente pretende ser un resumen explicativo del artículo, dónde se tratarán los temas más interesantes del mismo.

\subsection{Introducción}
Los enfoques actuales sobre el reconocimiento de imágenes ha hecho esencial el uso de técnicas de \textit{machinne learning} para la resolución de este tipo de problemas. Para mejorar los rendimientos que actualmente se pueden encontrar, nosotros podemos recolectar conjuntos de datos más grandes, entrenar modelos más potentes y usar mejores técnicas para evitar el sobreajuste de nuestro modelo. Hasta hace poco se contaban con conjuntos de datos (\textit{datasets}) relativamente pequeños, del orden de diez mil imágenes. Las tareas de reconocimiento simples pueden ser resueltas lo suficientemente bien con \textit{datasets} de este tamaño. Pero ahora nos encontramos con tareas más complejas y tenemos la posibilidad de trabajar con \textit{datasets} bastante más grandes. El \textit{datasets} nuevo más largo está incluido en LabelMe\cite{russell2008labelme} , el cuál consiste en un conjunto de imágenes de alta resolución con sus predicciones (\textit{labels}) y clasificadas en más de veintidós mil categorías.

Para poder entrenar tal cantidad de datos es necesario un modelo lo suficientemente grande y capaz de predecir esa cantidad de categorías. De todas formas, la inmensa complejidad del reconocimiento de objetos significa que este problema no pude ser especificado incluso por un \textit{dataset} tan largo como lo es ImageNet. Esto significa que tenemos que contar con conocimiento a priori para compensar todo los datos que no tenemos dentro del \textit{dataset}. Un modelo que encaje en este tipo de descripción es, sin lugar a dudas, una Red Neuronal Convolucional (CNN)(ver sección \ref{ConTeoCNN}). Su capacidad puede ser controlada variando su  amplitud y su profundidad y, además, crean fuertes y, en su mayoría, supuestos correctos sobre la naturaleza de las imágenes.

A pesar de las cualidades tan atractivas de las CNNs, estas no trabajan bien con imágenes de alta resolución porque resulta demasiado cara este tipo de ampliación. Afortunadamente, las GPUs (tarjetas gráficas) actuales vienen con una implementación altamente optimizada de convolución 2D, que son lo suficientemente potentes para facilitar el entrenamiento de CNNs particularmente grandes.

La res está entrenada con un subconjunto de datos de ImageNet usados en las competiciones ILSVRC-2010 y ILSVRC-2012\cite{berg2010large} y se han logrado resultados lo mejores resultado con bastante diferencia de los mejores reportados en este conjunto de datos. Se escribió una implementaciónn altamente optimizada para GPU de convolución 2D y todo el resto de operaciones que internamente se necesitan con las CNNs. La red contiene algunas características inusuales que mejoran el rendimiento y reducen el tiempo de entrenamiento. El tamaño de la red convierte al sobreajuste en un problema bastante serio, para solucionar esto se usan métodos para prevenir el sobreajuste. La red final contiene cinco CNNs y tres capas totalmente conectadas, esta profundidad parece ser importante ya que si se aumenta o disminuye el número de CNNs, entonces el rendimiento se reduce.

Al fin y al cabo, el tamaño de la red está limitado por la cantidad de memoria disponible en las GPUs actuales y por la cantidad de tiempo de entrenamiento que estamos dispuestos a tolerar. La red tarda de cinco a seis días para ser entrenada sobre dos tarjetas gráficas GTX 580 3GB. Todos los experimentos apuntan a que los resultados pueden ser mejorados con la mejora de las GPUs, haciéndolas más rápidas, y con \textit{datasets} más grandes.
\subsection{El dataset o conjunto de datos}
Se usa el un  subconjunto del \textit{dataset} ImageNet, que contiene sobre los quince millones de imágenes de alta resolución y están clasificadas con alrededor de veintidós mil categorías. El subconjunto que se usa es el que se utiliza en la competición llamada \textit{ImageNet Large-Scale Visual Recognition Challenge}(ILSVRC), y este contiene uno coma dos millones de imágenes de entrenamiento, cincuenta mil imágenes de validación y ciento cincuenta mil imágenes de \textit{test} o prueba.

ILSVRC-2010 es la única versión del ILSVRC  que tiene disponibles los \textit{labels} de las imágenes, por lo tanto este es el conjunto de datos sobre el que se han realizado la mayoría de los experimentos. eN ImageNet es posible mostrar los errores de dos formas: top-1 y top-5, dónde top-5 es el errror sobre las imágenes de test en el cual la predicción correcta no se encuentra dentro de las cinco clases más probables consideradas por el modelo.

ImageNet consiste en un conjunto de imágenes, cuya resolución es variable. Sin embargo, para el modelo, se necesitan imágenes con una resolución continua, osea, que todas las imágenes tengan la misma resolución (ver sección \ref{ConTeoCNN}). Por  lo tanto, se ha procedido a cambiar la resolución de las imágenes a una resolución común, 256X256. Las imágenes no se preprocesan de ninguna otra manera, excepto para extraer la actividad principal sobre el conjunto de entrenamiento a partir de cada pixel, por lo tanto, las imágenes son tratadas con los valores de sus filas RGB, se trabaja con los tres canales de color.
\subsection{Arquitectura}
La arquitectura de la red se puede ver en la imagen img. Contiene en total ocho capas de aprendizaje, de las cuales son cinco convolucionales y tres son capas totalmente conectadas (\textit{fully-connected}).
\subsubsection{ReLU de no linealidad}
El método normal para modelar la salida de una neurona como función aplicada sobre la entrada,
\begin{equation}
salida=f(entrada)
\end{equation} usando su función  de activación, es con la función tangente.
\begin{equation}
salida=tangente(entrada)
\end{equation}
En terminos de tiempo de entrenamiento con gradiente descendiente, estas saturaciones no lineales son mucho más lentas que si no usáramos saturamiento, con la función máximo.
\begin{equation}
salida=maximo(0,entrada)
\end{equation}
\figura{0.5}{imgs/TanMax.png}{Gráfico demostración de Saturación vs No Saturación}{Tan}{}
Nos referimos a las neuronas con esta no linearidad como \textit{Rectified Linear Units} (ReLUs), tal y cómo vemos en Nair and Hinton\cite{nair2010rectified}. Las Redes Neuronales Convolucionales Profundas se entrenan en un tiempo considerablemente menor que las que usan la función tangente. Esto se demuestra en la imagen \ref{Tan}, dónde se ve que entrenando una red pequeña, se necesita un número de iteraciones menor para llegar al 25\% de error de entrenamiento si no usamos el modelo con neuronas con saturación.

Este trabajo no es el primero en considerar el uso de de modelos de neuronas diferentes a los tradicionales en las CNNs. Pero el objetivo de estos otros trabajos era distinto y el principal objetivo de este conjunto de datos es prevenir el sobreajuste, el objetivo era distinto al de  hacer que la red se entrene de manera más rápida, lo que se pretende en este trabajo con sus ReLUs. El aprendizaje rápido tiene una buena influencia sobre el rendimiento sobre el entrenamiento de grandes modelos con grandes conjuntos de datos.
\subsubsection{Entrenamiento en múltiples GPUs}
El uso de una tarjeta gráfica GTX 580 que tiene sólo 3GB de memoria, limita el tamaño máximo de las redes que pueden ser entrenadas sobre esta. Si añadimos el problema de que con uno coma dos millones de ejemplos de entrenamiento son suficientes para que la red resultante sea demasiado grande como para que esta pueda ser entrenada sobre una sola GPU. Por lo tanto, el modelo ha sido entrenado sobre dos GPUs. Las GPUs actuales están bien preparadas para la paralelización, puesto que estas pueden acceder a la memoria de otra sin necesidad de pasar por la memoria principal del sistema. La paralelización usada en el modelo, básicamente entrena la mitad de neuronas en cada GPU, pero estaas solo pueden comunicarse con capas específicas de la otra GPU. Esto significa que si tenemos una capa 2 que se comunica con la capa 3 en su totalidad, estando estas dos en la misma capa, y tenemos una capa 4, esta se podrá conectar sólo con algunas neuronas de la capa 3, la capa 4 se encuentra en otra GPU. Elegir el patrón de comunicación es un problema para la validación cruzada pero hacerlo permite modificar la cantidad de comunicación  hasta que esta equivalga una fracción aceptable de cantidad de computo.

Como resultado la arquitectura que se obtiene es una arquitectura similar la CNN "columnar" empleada por Cire\c san \cite{cirecsan2011high},la cual tenía en su estructura capa opcional de preprocesado de imagen, capa convolucional, capa de tipo \textit{Max-Pooling} y una capa de clasificación; pero la diferencia es que las columnas de este modelo no son independientes. Esto reduce el error top-1 en 1,7\% y el error top-5 en 1,2\%. 
\section{Artículo del NeuralTalk}
\label{subchap:EstadoNeuralTalk}

\section{Artículo del Arctic Caption}
\label{subchap:EstadoArctic}

\section{Artículo del Google}
\label{subchap:EstadoGoogle}