\label{chap:EstadoArte}
En este capítulo se procede a la presentación de una serie de artículos que han conformado el estudio teórico que conlleva este proyecto, el cuál tiene un gran peso en el mismo; pues el estudio de cada una de las herramientas que se han tenido en cuenta tiene por detrás un concienzudo estudio por parte del alumno, que ha facilitado al alumno la compresión del funcionamiento de estas.

Para realizar la explicación de los artículos, los contenidos de estos se presentan de manera resumida, ordenada y eliminando los aspectos que sean demasiado avanzados o que no se consideren del todo necesaria su explicación en este proyecto.
\section{Artículo del DeepBelief SDK}
\label{subchap:EstadoDeepBelief}
El artículo de investigación que se va a explicar es el que se ha usado para implementar la herramienta \textit{Deep Belief SDK}\footnote{\url{https://github.com/jetpacapp/DeepBeliefSDK}} y se puede encontrar con el nombre de \textit{ImageNet Classification with Deep Convolutional Neural Networks} \cite{NIPS2012_4824}. Lo siguiente pretende ser un resumen explicativo del artículo, dónde se tratarán los temas más interesantes del mismo.

\subsection{Introducción}
Los enfoques actuales sobre el reconocimiento de imágenes ha hecho esencial el uso de técnicas de \textit{Machinne Learning} para la resolución de este tipo de problemas. Para mejorar los rendimientos que actualmente se pueden encontrar, se podrían recolectar conjuntos de datos más grandes, entrenar modelos más potentes y usar mejores técnicas para evitar el sobreajuste del modelo. Hasta hace poco se contaban con conjuntos de datos (\textit{datasets}) relativamente pequeños, del orden de diez mil imágenes. Las tareas de reconocimiento simples pueden ser resueltas lo suficientemente bien con \textit{datasets} de este tamaño. Pero ahora existen tareas más complejas y la posibilidad de trabajar con \textit{datasets} bastante más grandes. El \textit{datasets} nuevo más largo está incluido en LabelMe \cite{russell2008labelme}, el cuál consiste en un conjunto de imágenes de alta resolución con sus predicciones (\textit{labels}) y clasificadas en más de veintidós mil categorías.

Para poder entrenar tal cantidad de datos es necesario un modelo lo suficientemente grande y capaz de predecir esa cantidad de categorías. De todas formas, la inmensa complejidad del reconocimiento de objetos significa que este problema no puede ser especificado incluso por un \textit{dataset} tan largo como lo es ImageNet. Esto significa que los autores del proyecto tuvieron que contar con conocimiento a priori para compensar todo los datos que no tenían disponibles dentro del \textit{dataset}. Un modelo que encaje en este tipo de descripción es, sin lugar a dudas, una Red Neuronal Convolucional (CNN) (ver sección \ref{ConTeoCNN}). Su capacidad puede ser controlada variando su  amplitud y su profundidad y, además, crean fuertes y, en su mayoría, supuestos correctos sobre la naturaleza de las imágenes.

A pesar de las cualidades tan atractivas de las CNNs, estas no trabajan bien con imágenes de alta resolución porque resulta demasiado cara este tipo de ampliación. Pero pudieron resolver este problema debido a que las GPUs (tarjetas gráficas) actuales vienen con una implementación altamente optimizada de convolución 2D, que son lo suficientemente potentes para facilitar el entrenamiento de CNNs particularmente grandes.

La red está entrenada con un subconjunto de datos de ImageNet usados en las competiciones ILSVRC-2010 y ILSVRC-2012 \cite{berg2010large} y se han logrado resultados bastante mejores que los que han sido obtenidos hasta entonces sobre ese conjunto de datos. El modelo del artículo posee una implementación altamente optimizada para GPU de convolución 2D y todo el resto de operaciones que internamente se necesitan con las CNNs. La red contiene algunas características inusuales que mejoran el rendimiento y reducen el tiempo de entrenamiento. El tamaño de la red convierte al sobreajuste en un problema bastante serio, para solucionar esto se usan métodos para prevenir el sobreajuste. La red final contiene cinco CNNs y tres capas totalmente conectadas, esta profundidad parece ser importante ya que si se aumenta o disminuye el número de CNNs, entonces el rendimiento se reduce.

En al artículo se cuenta que un gran inconveniente se encuentra en que el tamaño de la red está limitado por la cantidad de memoria disponible en las GPUs actuales y por la cantidad de tiempo de entrenamiento se están dispuestos a tolerar. La red tarda de cinco a seis días para ser entrenada sobre dos tarjetas gráficas GTX 580 3GB. Todos los experimentos que realizaron apuntan a que los resultados pueden ser mejorados con la mejora de las GPUs, haciéndolas más rápidas, y con \textit{datasets} más grandes.
\subsection{El dataset o conjunto de datos}
Se usa el un  subconjunto del \textit{dataset} ImageNet, que contiene sobre los quince millones de imágenes de alta resolución y están clasificadas con alrededor de veintidós mil categorías. El subconjunto que se usa es el que se utiliza en la competición llamada \textit{ImageNet Large-Scale Visual Recognition Challenge} (ILSVRC), y este contiene uno coma dos millones de imágenes de entrenamiento, cincuenta mil imágenes de validación y ciento cincuenta mil imágenes de \textit{test} o prueba.

ILSVRC-2010 es la única versión del ILSVRC  que tiene disponibles las etiquetas de las imágenes, por lo tanto este es el conjunto de datos sobre el que se han realizado la mayoría de los experimentos. En ImageNet es posible mostrar los errores de dos formas: top-1 y top-5, dónde top-5 es el error sobre las imágenes de test en el cual la predicción correcta no se encuentra dentro de las cinco clases más probables consideradas por el modelo.

ImageNet consiste en un conjunto de imágenes, cuyo tamaño es variable. Sin embargo, para el modelo, se necesitan imágenes con un tamaño fijo, osea, que todas las imágenes tengan el mismo tamaño (ver sección \ref{ConTeoCNN}). Por  lo tanto, para solucionar este problema, cambiaron el tamaño de las imágenes a un tamaño común, 256X256. Las imágenes no se preprocesan de ninguna otra manera, excepto para extraer la actividad principal sobre el conjunto de entrenamiento a partir de cada pixel, por lo tanto, las imágenes son tratadas con los valores de sus filas RGB, se trabaja con los tres canales de color.
\subsection{Arquitectura}
Contiene en total ocho capas de aprendizaje, de las cuales cinco son convolucionales y tres son capas totalmente conectadas (\textit{fully-connected}).
\subsubsection{ReLU de no linealidad}
El método normal para modelar la salida de una neurona como función aplicada sobre la entrada es:
\begin{equation}
salida=f(entrada)
\end{equation} donde la función  de activación utilizada es con la función tangente.
\begin{equation}
salida=tangente(entrada)
\end{equation}
En terminos de tiempo de entrenamiento con gradiente descendiente, estas saturaciones no lineales son mucho más lentas que si no usáramos saturamiento, con la función máximo.
\begin{equation}
salida=máximo(0,entrada)
\end{equation}
\figura{0.5}{imgs/TanMax.png}{Gráfico demostración de Saturación vs No Saturación}{Tan}{}
Las neuronas con esta no linearidad se conocen como \textit{Rectified Linear Units} (ReLUs), tal y cómo se puede ver en Nair and Hinton \cite{nair2010rectified}. Las Redes Neuronales Convolucionales Profundas se entrenan en un tiempo considerablemente menor que las que usan la función tangente. Esto se demuestra en la imagen \ref{Tan}, dónde se ve que entrenando una red pequeña, se necesita un número de iteraciones menor para llegar al 25\% de error de entrenamiento si no usamos el modelo con neuronas con saturación.

El trabajo del artículo no es el primero en considerar el uso de de modelos de neuronas diferentes a los tradicionales en las CNNs. Pero el objetivo de estos otros trabajos era distinto y el principal objetivo de este conjunto de datos es prevenir el sobreajuste, el objetivo era distinto al de  hacer que la red se entrene de manera más rápida, lo que se pretende en el artículo con sus ReLUs. El aprendizaje rápido tiene una buena influencia sobre el rendimiento sobre el entrenamiento de grandes modelos con grandes conjuntos de datos.
\subsubsection{Entrenamiento en múltiples GPUs}
El uso de una tarjeta gráfica GTX 580, tal y como se comenta en el artículo, que tiene sólo 3GB de memoria, limita el tamaño máximo de las redes que pueden ser entrenadas sobre esta. Si se añade el problema de que trabajaron con conjunto de datos con uno coma dos millones de ejemplos de entrenamiento, los cuales son suficientes para que la red resultante sea demasiado grande como para que esta pueda ser entrenada sobre una sola GPU, entonces el modelo era inabarcable por una sola tarjeta gráfica. Por lo tanto, el modelo tuvo que ser entrenado sobre dos GPUs. Las GPUs actuales están bien preparadas para la paralelización, puesto que estas pueden acceder a la memoria de otra sin necesidad de pasar por la memoria principal del sistema. La paralelización usada en el modelo, básicamente entrena la mitad de neuronas en cada GPU, pero estas solo pueden comunicarse con capas específicas de la otra GPU. Por lo que la decisión de qué capa se comunicaba con qué otra capa, se convirtió en un problema a resolver, pero esto permitía reducir la carga computacional de la comunicación hasta un valor aceptable.

Como resultado, la arquitectura que obtuvieron es una arquitectura similar la CNN ``columnar'' empleada por Cire\c san \cite{cirecsan2011high}, la cual tenía en su estructura una capa opcional de preprocesado de imagen, una capa convolucional, una capa de tipo \textit{Max-Pooling} y una capa de clasificación; pero la diferencia es que las columnas del modelo presentado en el artículo no son independientes. Esto reduce el error top-1 en 1,7\% y el error top-5 en 1,2\%.
\subsubsection{Respuesta local a la Normalización}
Lo más destacable de este apartado es que al  utilizar neuronas de tipo ReLU en el modelo del artículo, la normalización no es necesaria para evitar el saturamiento de las neuronas.
\subsubsection{Overlapping Pooling o Puesta en común superpuesta}
El trabajo de las capas de \textit{pooling}, o puesta en común, es la de resumir las salidas de los grupos de neuronas vecinos que le corresponde. Tradicionalmente los vecindarios de neuronas al ser resumidos no se superponían. Para que quede más claro, una capa de \textit{pooling} está formada por una cuadrícula de unidades de \textit{pooling} que están separadas entre sí por un número $X$ de píxeles, y cada resumen realizado se hace sobre un vecindario de tamaño $Y\times Y$. 
\figura{0.7}{imgs/ReLU.png}{Overlapping Pooling}{Over}{}
Si la separación entre unidades de \textit{pooling} es igual que $Y$, entonces la capa de puesta en común es la tradicional; pero si nos encontramos con que la separación es menor que el valor $Y$, entonces las unidades de \textit{pooling} se superponen al resumir vecindarios y así obtenemos una capa de puesta en común superpuesta (ver \ref{Over}.

En conclusión, se obtiene que el error en top-1 y top-5 se redujo en 0.4\% y 0.3\%, respectivamente. Por eso los autores del artículo decidieron usar unos valores de dos para $X$ y de tres para $Y$.
\subsubsection{Arquitectura}
Como se ha dicho antes, la arquitectura de la red está formada por ocho capas, de las cuáles las cinco primeras con convolucionales y las tres restantes son tres capas completamente conectadas. La salida de la última capa completamente conectada es una matriz de 1000 datos, que se corresponde con las 1000 \textit{labels} de clase que tenemos.

Las capas dos, cuatro y cinco convolucionales están únicamente conectadas con la capa anterior, la cual está situada en la misma GPU. La capa 3, por el contrario, está completamente conectada con la segunda capa. Capas de normalización están después de la primera y segunda capa convolucional. Capas de \textit{pooling} están después de las de normalización y después de la quina capa convolucional. La ReLU, función de no linealidad, se aplica en todas las capas, tanto convolucionales como las completamente  conectadas.

\subsubsection{Reduciendo el sobreajuste}
Debido a la cantidad de parámetros de la red neuronal, unos sesenta millones, y a la cantidad de clases que contiene el conjunto de datos, se hace imposible entrenar una red sin poder tener en cuenta la probabilidad de que se produzca un sobreajuste.

Para evitar el sobreajuste de la red hemos usado dos métodos:
\begin{itemize}
	\item El primero es aumentar el conjunto de datos original, esto se hace creando nuevas imágenes a partir de las imágenes originales pero conservando su etiqueta. Las dos formas para hacer esto que se ha usado en este artículos son:
	\begin{itemize}
		\item Hacer traslaciones en la imagen y reflexiones horizontales a través de los cuáles obtenemos una nueva imagen.
		\item Se ha aumentado la intensidad de los canales RGB de la imagen para obtener otra imagen que sea distinta para el modelo.
	\end{itemize}
	\item Se usa un nuevo método llamado \textit{dropout} el que consiste en poner a cero la salida de las neuronas de la capa oculta con una probabilidad de 0.5. Las neuronas que sean abandonadas en este proceso, entonces no participaran en la propagación hacia atrás.
\end{itemize}
\subsubsection{Detalles del aprendizaje}
En el artículo se explica que se inicializan los pesos de cada capa a partir de una distribución Gaussiana con una desviación estándar de 0.01. Los sesgos de las neuronas en las capas dos, cuatro y cinco los determinaron con un valor constante de 1. Esta inicialización lo que hace, según se comenta en el artículo, es acelerar el proceso de aprendizaje en los pasos tempranos dando a las ReLUs entradas positivas. En el resto de capas los sesgos se inicializan a cero.

Los autores han usado el ratio de aprendizaje igual para todas las capas y este es ajustado de forma manual. La heurística que siguieron es que el ratio de aprendizaje es dividido entre diez cuanto el ratio del error de validación deja de mejorar con el ratio de aprendizaje actual. El ratio de entrenamiento se inicializa a 0.01. La red la entrenaron en 90 ciclos con un conjunto de imágenes de 1.2 millones de tamaño, el cuál tomó 5 o 6 días en finalizar sobre las dos tarjetas gráficas ya mencionadas.
\subsubsection{Resultados}
El resultado sobre el dataset ILSVRC-2010 ha conseguido, sobre el conjunto de test, unos ratio de error top-1 y top-5 de 37,5\% y 17\%. Teniendo en cuenta que en la competición de 2010 se consiguió el mejor rendimiento como un 47,1\% y de 28.2\%, se puede decir que este modelo tiene bastante mejor resultados.
\subsubsection{Conclusión}
Las conclusiones más importantes que se pueden sacar del artículo es que una red neuronal convolucional profunda, siempre que sea lo suficientemente grande, es capaz de romper los récords sobre los resultados ya existentes. También es interesante el hecho de que si alguna de las capas convolucionales es quitada o añadida a este modelo, entonces su error ratio se incrementa; por alguna razón la profundidad de la red es importante a la hora de mejorar los ratio de error.
\section{Artículo del NeuralTalk}
\label{subchap:EstadoNeuralTalk}
En este apartado se procederá a resumir el artículo asociado a la herramienta NeuralTalk \cite{karpathy2014deep}.
\subsection{Introducción}
Una vistazo rápido es suficiente para el humano para poder extraer una inmensa cantidad de detalles de la escena que este presenciando \cite{iyerwe}. Sin embargo está tarea es muy compleja para nuestros modelos de reconocimiento visual. Hasta ahora los esfuerzos en el reconocimiento de imágenes ha sido  el de etiquetar imágenes a través de un conjunto fijo de categorías, el cuál ha progresado bastante. Sin embargo, estos métodos tienen un vocabulario muy restrictivo en comparación con el vocabulario descriptivo que tiene el ser humano.

Algunos pioneros se acercan a resolver el reto de generar descripciones de imágenes. Sin embargo, estos modelos usualmente tienen ciertas deficiencias que les impide alcanzar una gran variedad en el reconocimiento. Por otra parte, el tema central de estos trabajos ha sido reducir la complejidad de las imágenes en una sola sentencia.

\figura{0.7}{imgs/neural1.png}{Asociación de imágenes a regiones del espacio}{neural1}{}

El trabajo presentado en el artículo tiene el objetivo de generar descripciones complejas (ver Figura \ref{neural1}) a partir de una imagen. El principal reto del trabajo fue diseñar un modelo lo suficientemente rico para a la vez razonar sobre el contenido de la imagen y su representación en lenguaje natural. El segundo reto fue el de encontrar un \textit{dataset} lo suficientemente grande sobre el que trabajar.

La idea central que se propone en el artículo es aprovechar estos grandes conjuntos de datos mediante el tratamiento de las frases como etiquetas débiles, en la que los segmentos contiguos de palabras corresponden a algunos en particular, pero la ubicación es desconocida en la imagen. Para esto, los autores tuvieron que realizar dos aportaciones al modelo, que son las siguientes:
\begin{itemize}
	\item Desarrollaron un modelo de red neuronal que es capaz de relacionar los segmentos de frases con la región de la imagen que esta describiendo.
	\item Introdujeron una arquitectura de red neuronal recurrente multimodal que es capaz de generar una descripcion en texto a partir de una imagen que toma como entrada.
\end{itemize}
\subsection{El modelo}
\figura{1}{imgs/neural2.png}{Generación de descripciones de regiones de la imagen}{neural2}{}
El objetivo final del modelo presentado en el artículo es generar descripciones de regiones de imágenes. Durante el entrenamiento, la entrada del modelo es un conjunto de imágenes y sus correspondientes etiquetas, que son frases en lenguaje natural (ver Figura \ref{neural2}). En primer lugar se presenta un modelo que asocia fragmentos de oraciones a regiones de la imagen. A continuación, se tratan estas asociaciones como datos de entrenamiento para una segunda red, que aprende a generar los fragmentos de las oraciones.
\subsubsection{Aprendiendo a relacionar datos visuales con datos de lenguaje}
El modelo de alineación necesita una entrada de un conjunto de imágenes con sus respectivas frases descriptivas. La idea principal de este modelo es que las personas puede escribir frases referentes a la imagen, pero no sabemos a que parte de la imagen se refieren estas. Asumieron, entonces, que existe una relación entre la frase y el objeto que se describe de la imagen; por tanto, procedieron a intentar encontrar estas relaciones para posteriormente aprender a generar estos fragmentos de imágenes a los que las oraciones se refieren.

Primero se creó una red neuronal que asocia las palabras con las regiones de la imagen. Entonces, el siguiente objetivo que se propusieron fue relacionar semánticamente estas palabras.
\subsubsection{Representando imágenes}
Los autores observaron que las frases hacen referencias frecuentes a los atributos de los objetos que están describiendo. Así, siguiendo el método de Girshick \cite{girshick2014rich} para la detección de objetos en todas las imágenes, usaron una Red Neuronal Convolucional. La CNN, tal y como se comenta en el artículo, fue pre-entrenada con el \textit{dataset} de ImageNet \cite{deng2009imagenet}, y afinada sobre las 200 clases del ImageNet Detection Challenge \cite{russakovsky2014imagenet}.
\subsubsection{Representando frases}
\figura{0.7}{imgs/neural3.png}{Diagrama del modelo NeuralTalk}{neural3}{}
Para establecer las relaciones inter-modales,pensaron que sería conveniente representar las palabras de las frases en el mismo espacio dimensional que ocupan en la región de la imagen. Comentan en el artículo que el enfoque más simple podría ser proyectar cada palabra en este espacio. Debido a que lo anterior presenta ciertos defectos, determinaron que se podría usar una extensión de este método, que fue el uso de bigramas de palabras o el uso de relaciones de dependencia. Sin embargo, esto sigue imponiendo un tamaño máximo arbitrario de la imagen y requiere el uso de Árboles de Dependencia, que debe ser entrenada con texto no relacionado (ver Figura \ref{neural3}).

Para conseguir este propósito en el modelo del artículo, se uso una Red Neuronal Recurrente Bidireccional (ver sección \ref{ConTeoRNN}) para computar las representaciones de las palabras.
\begin{itemize}
\item Objetivo de alineamiento:
\subitem Se ha descrito las transformaciones que asocian a cada imagen con la frase en un conjunto de vectores en común, dentro de un mismo espacio dimensional. Como la extracción y la predicción se realizan sobre la imagen y la frase entera, se creó una puntuación imagen-frase como una función de las puntuaciones región-palabra individuales. Intuitivamente, una predicción del tipo imagen-frase, tendrá una puntuación elevada si sus puntuaciones región-palabra son elevadas, osea se relacionan bien y tienen buen soporte.
\item Decodificando segmentos de texto alineados a imágenes:
\subitem Lo que se pretende es generar secuencias de palabras que estén asociadas a una imagen, no una palabra suelta asociada a la misma; además estas deben estar relacionadas semánticamente entre ellas, por lo tanto se meten en una <<caja>> para posteriormente generar la secuencia con esa relación semántica exigida.
\end{itemize}
\subsubsection{Red Neuronal Recurrente Multimodal para la generación de descripciones}
En esta sección se toma en cuenta que la entrada sera un conjunto de imágenes y sus correspondientes descripciones. Estas pueden ser imágenes y su frase descriptiva o regiones y fragmentos de texto, como se ha comentado anteriormente. El desafío clave es diseñar un modelo que pueda sacar como predicción a través de una imagen, que consiste en una secuencia variable (la frase que describa la imagen). En anteriores trabajos basados en RNN, esto se conseguía obteniendo una probabilidad de cuál sería la siguiente palabra en una secuencia, teniendo disponible la palabra actual y el contexto anterior (dado por las conexiones recurrentes).En el modelo del artículo se usa una pequeña extensión de estas redes.
\begin{itemize}
\item Entrenando la RNN:
\subitem LA RNN está entrenada para predecir la siguiente palabra usando la palabra actual y el contexto en el tiempo anterior. Se condiciona las predicciones de la RNN a través de la interacción con su umbral en los primeros pasos de la predicción. El entrenamiento funciona de la siguiente manera: se empieza con una palabra especial para determinar el comienzo de la predicción, y la primera predicción obtenida será la primera palabra de la frase a obtener. La siguiente palabra se traduce tomando la palabra actual como la recién predicha y el contexto anterior, esperando que el modelo prediga la siguiente palabra como la segunda; y así sucesivamente. Cuando se llega a la última palabra, la palabra predicha será una palabra reservada para determinar el fin de la predicción. Para aclarar esto no ayudaremos de la Figura \ref{neural4}.
\figura{0.7}{imgs/neural4.png}{Diagrama de la RNN del modelo NeuralTalk}{neural4}{}

\item Testeando la RNN:
\subitem Para predecir una frase, se computa la representación de la imagen y se empieza con la palabra reservada que determina el comienzo de la predicción. Se muestra una palabra de la distribución, que se toma como palabra actual y se procede a predecir la siguiente palabra; esto se repite hasta obtener la palabra reservada que determina el fin de la frase.
\end{itemize}

\subsubsection{Conclusiones}
Crearon un modelo capaz de generar frases a partir de imágenes, pero tiene la gran limitación de que estas sólo pueden tener una resolución fija. Por último, hay que tener en cuenta que este modelo en realidad consiste en dos modelos, uno que preprocesa las imágenes para extraer las palabras asociadas a regiones de la imagen y el segundo es la RNN que predice las frases.

\section{Artículo del Arctic Caption}
\label{subchap:EstadoArctic}
En este apartado se hablará del proyecto Arctic Caption, para ello se va a proceder a resumir el artículo que viene asociado a este \cite{xu2015show}.

\subsection{Introducción}
La generación de frases automáticamente a través de una imagen dada es  una tarea muy cercana al corazón del entendimiento de una escena, que es uno de las primeras metas de la visión por computadora. No sólo los modelos que realizan estas operaciones deben ser lo suficientemente potentes para enfrentar la carga computacional que lleva el hecho de detectar qué objetos hay en una imagen, sino que también deben ser capaz de relacionar estos objetos a través del lenguaje natural. Por estas razonas la generación de frases ha sido considerado como un problema de extrema dificultad. Es uno de los retos más importantes del \textit{Machine Learning}, el imitar la capacidad humana de comprender grandes cantidades de información visual y transcribirla a lenguaje natural.

A pesar de la complicación que esta tarea conlleva, ha habido un reciente aumento de estudios que están tratando de resolver este tipo de problem. Esto se debe a la mejora en los algoritmos de aprendizaje máquina y el aumento de conjuntos de datos sobre los que poder trabajar. Trabajos recientes han mejorado la calidad de la generación de frases a través de la combinación de CNNs, que obtienen una representación vectorial de la imagen, y de RNNs, que decodifican las representaciones de las CNN y las convierten en frases en lenguaje natural.

El humano tiene la capacidad de centrar la atención sólo en ciertas partes de una imagen, gracias a esto es capaz de trabajar incluso con imágenes muy desordenadas. En trabajos anteriores se ha usado las CNNs para la extracción de estas imágenes y con esto formar la frase, lo que ha proporcionado un resultado bastante satisfactorio. Pero, esto tiene una desventaja y es la pérdida de información que pudiera llegar a ser necesaria para formar frases aún más descriptivas. Para evitar esta pérdida de información se podrían usar un mayor número de representaciones a bajo nivel. Sin embargo, hay que encontrar un modelo y un mecanismo que sea capaz de trabajar con todas estas características.

En este artículo se explica un intento de insertar una forma de mecanismo de atención en el reconocimiento de las imágenes, este mecanismo tiene dos variantes: un mecanismo de atención <<fuerte>> y un mecanismo de atención <<débil>>. 

Las aportaciones de este artículo son las siguientes:
\begin{itemize}
	\item Se introduce dos generadores de frase basados en  la atención hacia la imagen en un mismo  \textit{framework}:
	\begin{itemize}
		\item Un mecanismo de atención determinista <<débil>>, que puede ser entrenado con técnicas normales de propagación hacia atrás.
		\item Un método de atención estocástico <<fuerte>>, que puede entrenarse con el método descrito en el artículo de Ronald J. Williams \cite{williams1992simple}, que se conoce por el nombre de REINFORCE.
	\end{itemize}
	\item Se muestra cómo interpretar los resultados, visualizando dónde y en qué se centro la atención.
	\item Finalmente se valida cuantitativamente la utilidad de la atención en la generación de frases. Se hace a través de tres conjuntos de datos.
\end{itemize}
\subsection{Generación de Frases de Imágenes con un Mecanismo de Atención}
\subsubsection{Detalles del modelo}
En esta sección se describe las dos variantes del modelo de atención de las que se ha hablado previamente. La principal diferencia se encuentra en la función de activación de las neuronas.

Para detallar el modelo, se ha dividido la sección en dos partes:
\begin{itemize}
	\item 1- Codificador: Características convolucionales
	\subitem El modelo toma una imagen en forma de vector y genera una frase $Y$ codificada como una secuencia de 1 a $K$ palabras codificadas. 
	\begin{equation}
	  y=\{y_{1},...,y_{c}\}, \forall y_{i}\in{ \mathbb{R}^{K}}
	    \end{equation}
	    En la ecuación, $K$ es el tamaño del lenguaje, el número de posibles palabras. $C$ es la longitud de la frase. $Y$ es la frase formada por una serie de palabras.
	    
	    En el artículo se ha usado una CNN para extraer el conjunto de vectores de características de la imagen. El extractor de características produce $L$ número de vectores, cada uno de los cuáles es una representación $D$-dimensional correspondiente a una parte de la imagen.
	    \begin{equation}
	    a=\{a_{1},...,a_{L}\}, \forall a_{i}\in{ \mathbb{R}^{D}}
	    \end{equation}
	    Con la intención de obtener la correspondencia de los vectores de características y las porciones de la imagen, se extrae las características de la capa convolucional más baja a diferencia de los trabajos previos, que usaban una capa totalmente conectada. Esto permite al decodificador centrarse de forma selectiva en ciertas partes de la imagen seleccionado un subconjunto de todos los vectores de características, osea un subvector del vector a de la ecuación anterior.
	\item 2- Decodificador: Red de Memoria Largo Corto Plazo (LSTM)
	\subitem El artículo explica que el modelo usa una LTSM, que produce un frase a partir de un vector de contexto, el estado de la capa oculta anterior y las palabras generadas anteriormente. 
	La LTSM trabaja de manera que, para cada región de la imagen genera un peso positivo que puede ser interpretado como la probabilidad de que la región en la que nos estamos centrando sea la correcta para generar la siguiente palabra. Este peso es computado por el modelo de atención,  para el que se ha usado un Perceptrón Multicapa que está condicionado por el estado de su capa previa. Hay que destacar que el estado de la capa previa varia en función de las palabras que ya han sido generadas.	
\end{itemize}
\subsection{Conclusiones}
Se ha propuesto un modelo de generación de frases a partir de imágenes que está basado en un método de atención, el cual puede ser fuerte o débil. Cabe destacar que los autores del artículo esperan que este tipo de trabajos incentiven a futuros trabajos a utilizar una metodología basada en la atención en las imágenes, la cuál está basada en la capacidad de el ser humano de prestar atención a ciertas partes de la imagen.
\section{Aplicaciones existentes relacionadas con el proyecto}
Como ya hemos comentado en la introducción (ver Capítulo \ref{chap:Introduccion}), este proyecto tiene una dimensión teórica y otra práctica, ahora se procederá a comentar aplicaciones similares a la desarrollada en este proyecto para cubrir el estado del arte de la dimensión práctica \cite{artePrac}.

\subsection{TapTapSee}
TapTapSee es una aplicación gratuita para IOS, la cuál posee una interfaz muy simple. La aplicación cuenta con tres botones, uno en la parte superior derecha de la pantalla, otro en la superior izquierda y el otro botón ocupa el resto de la pantalla, pues es el más importante.

Esta aplicación te permite reconocer objetos. Debes apuntar la cámara en la dirección del objeto a reconocer y tocar dos veces la pantalla, seguidamente una voz te informará de que se ha tomado una foto. La foto se envía a un servidor, donde se busca una coincidencia y, cuando esta es encontrada, se te comunica en voz alta qué objeto se encuentra en la foto.

Es importante destacar que no tienes por qué esperar a que la aplicación termine  de reconocer el objeto para tomar otra foto, pues esta te permite tomar hasta cinco fotos de manera consecutiva sin que la primera haya sido reconocida aún. Esta es una utilidad muy destacable, pues convierte a la aplicación en una herramienta más potente.

Esta aplicación usa una combinación de una base de datos de imágenes y humanos para realizar el  reconocimiento. Además, la misma foto puede devolver resultados diferentes en distintas ejecuciones del reconocimiento.

En conclusión, esta aplicación es una herramienta útil, pero no es comparable al objetivo que nuestra aplicación prototipo tiene, pues no se trata de un simple reconocimiento de imágenes, sino de la generación de una descripción basada en la imagen que se ha tomado como referencia.

\subsection{CamFind}
CamFind está desarrollada por la misma empresa que desarrolló la aplicación TapTapSee. Esto es debido a la experiencia que ganaron los desarrolladores al crear TapTapSee y a la cantidad de comentarios que recibieron de la comunidad, que sirvieron como retroalimentación para el desarrollo de CamFind.

CamFind no posee una interfaz tan sencilla como TapTapSee, pero tiene muchas más funcionalidades. CamFind usa el mismo sistema de reconocimiento que TapTapSee, por lo que sus resultados tienden a ser similares.

La utilidad más destacable de CamFind es que a partir de una foto, no sólo reconoce el objeto que se ha tomado, sino que, además, te ofrece información sobre el precio de dicho producto en Internet, comparación de precios, objetos relacionados, etc\dots Esta utilidad la convierte en una herramienta muy útil y potente.

Este tipo de herramienta es bastante útil, pero ha perdido el objetivo de apoyo a personas con dificultades de visión porque, al tener tantas funcionalidades, su interfaz se hace demasiada completa para que una persona con dificultades de visión pueda usarla con asiduidad y facilidad. Además, sigue sin acercarse al objetivo de nuestra aplicación prototipo, cuya tarea es mucho más compleja que la de este tipo de aplicaciones.

\subsection{Talking Goggles}
Talking Goggles usa la base de datos de imágenes Goggles de Google para hacer su reconocimiento.

Lo más interesante de Talking Goggles  es que puedes poner el modo cámara de vídeo y ejecutar predicciones en tiempo real, aunque la aplicación sólo detectará unos pocos objetos. Además, el nivel de error de Talking Goggles es aún bastante alto, pues hay bastantes objetos que reconoce de manera incorrecta o que, simplemente, no los reconoce.

Cabe destacar que posee una interfaz muy sencilla, que consta de cuatro botones, que dividen la pantalla en cuatro trozos iguales.

Esta aplicación también usa una guía por voz que, en el caso de esta aplicación, es la suya propia, ha sido desarrollada específicamente para su uso en Talking Goggles.

En conclusión, podemos ver en Talking Goggles una aplicación potente y competitiva con el mercado actual de aplicaciones de este tipo pero, nuestra aplicación renueva todo lo que se encuentra en el mercado actual, ya que ninguna aplicación puede generar descripciones a partir de imágenes. Otro punto importante es que las técnicas y herramientas usadas en la aplicación prototipo apenas pueden llegar a superar el año de edad, por lo tanto, es una aplicación bastante innovadora.

